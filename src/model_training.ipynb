{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「modelTraining_ipynb」的副本.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [[ Model Training ]]\n",
        "Train a CNN performing the image classification task on the MNIST dataset.\n",
        "<img src=https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png width=500>\n",
        "\n",
        "- Use PyTorch to train NN models.\n",
        "- Do quantization aware training.\n",
        "- Try different hyper-parameters and model architecures and see the effects."
      ],
      "metadata": {
        "id": "S6Igz42wEGDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Models\n",
        "The model we are constructing consists of 1 convolution layer and 3 fully connected (FC) layers.\n",
        "\n",
        "![nn_s.png](data:image/png;base64,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)\n",
        "\n",
        "- The convolution layer has 3 output channels and the kernel size is 3, the stride is 2 and the padding is 0.\n",
        "- A batch normalization (BN) layer is put after the convolution layer.\n",
        "- Following the BN layer are three FC layers each with `numNeurons`, `numNeurons`, and 10 neurons, respectively.\n",
        "- The activation function will be relu6, which is defined as $max(0,min(6,x))$.\n",
        "\n",
        "We will define both the floating-point and quantized model with the above architecture."
      ],
      "metadata": {
        "id": "ssO1Mo_ACU5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The quantized model\n",
        "\n",
        "We then define the quantized model.\n",
        "As will will implement quantization aware training, we have to first define the \"fake quantization\" operation, and then use it to define quantized layers.\n",
        "Finally, we will use the quantized layers to build a 4-bit quantized model."
      ],
      "metadata": {
        "id": "c72W4Z-WBQfS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvHRS9FRD2mr"
      },
      "source": [
        "### Define \"fake quantization\" operation\n",
        "Before define the quantized model, we first prepare the \"fake quantization\" operation. It quantization the input tensor and use straight through estimator when computing the gradient for back proagation.\n",
        "\n",
        "- To define operations with customized gradient, we have to declare a calss inheriting `torch.autograd.Function` and implement the `forward` and `backward` functions.\n",
        "- In `forward` we `quantize_tensor` and then `dequantize_tensor`. Since there are rounding and clamping in `quantize_tensor`, the values in the tenssor will be changed.\n",
        "- In `backward`, we define how the gradient is computed. Since we use the straight through estimator, we don't need to modify the gradient and can directly return it.\n",
        "- More information can be found [here](https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd).\n",
        "\n",
        "In `quantize_tensor`, we basically rescale the values of the input tensor to $[-(2^{n-1}),2^{n-1}-1]$, where $n$ is the bitwidth for quantization, by multiplying the input tensor with `scale` shown in the following code.\n",
        "- If `imposePow` is not specified:\n",
        "    - `quantize_tensor` will compute the value of `scale`\n",
        "    - Used when quantizing the weights and biases where we don't know the quantization scheme beforehand.\n",
        "- If `imposePow` is specified:\n",
        "    - `scale` is set according to `imposePow`\n",
        "    - Used when quantizing the activations where we know the quantization scheme since we designed to use 4 bits for the interger part due to the relu6 activation function.\n",
        "\n",
        "The `roundType` is relevant to the hardware implementation.\n",
        "- For weights, we use `'round'` to get less quantization error.\n",
        "- For activations, we use `'floor'` since the circuit we are going to used will truncate the LSBs of activations, which corresponds to take the floor operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGViX8PKEn_e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
        "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None,imposePow=None,roundType='round'):\n",
        "    if imposePow is not None:\n",
        "        scale=2**(-1*imposePow)\n",
        "    else:\n",
        "        if not min_val and not max_val: \n",
        "            min_val, max_val = x.min(), x.max()\n",
        "        if min_val!=max_val:\n",
        "            scale=2**(torch.ceil(torch.log2((max_val-min_val)/(2.**(num_bits)-1.))))\n",
        "        else:\n",
        "            scale=2**(torch.ceil(torch.log2((torch.tensor([1e-4]))/(2.**(num_bits)-1.))))\n",
        "    zero_point=0\n",
        "    q_x = zero_point + x / scale\n",
        "    qmin = (2.**(num_bits-1))*(-1.)\n",
        "    qmax = 2.**(num_bits-1) - 1.\n",
        "    if roundType=='round':\n",
        "        q_x.clamp_(qmin, qmax).round_()\n",
        "    elif roundType=='floor':\n",
        "        q_x.clamp_(qmin, qmax).floor_()\n",
        "    \n",
        "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
        "\n",
        "def dequantize_tensor(q_x):\n",
        "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
        "\n",
        "class FakeQuantOp(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, w, numBits=4,imposePow=None,roundType='round'):\n",
        "        x = quantize_tensor(w,num_bits=numBits,imposePow=imposePow,roundType=roundType)\n",
        "        x = dequantize_tensor(x)\n",
        "        return x\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # straight through estimator\n",
        "        return grad_output, None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CxqzJJaG0OY"
      },
      "source": [
        "### Define quantized layers\n",
        "Then, we define two kinds of quantized layers that will be used to build our quantized model. To define a layer, we also create a calss inheriting `nn.Module` and define the inference behaviors in `forward` as we did when defining a mdoel.\n",
        "\n",
        "The two types of layers are:\n",
        "- `ConvBN_Quant`: A fused layer including convolution and batch normalization layers that supports quantization aware training (QAT).\n",
        "- `Linear_Quant`: A linear layer that supports QAT.\n",
        "\n",
        "In either layer type, we declare corresponding layer(s) offered in PyTorch as class members in `init`. When forwarding, we do the following steps:\n",
        "1. extract the weights of the layer(s)\n",
        "2. for `ConvBN_Quant`, fold the weights of conv and BN\n",
        "3. apply `FakeQuantOp` on the weights\n",
        "4. perform the `conv2d` or the `linear` operation with the quantized weights\n",
        "\n",
        "- Since the quantization won't change the layer parameters, we effectively keep a floating-point copy of the weights.\n",
        "- The inference is based on the quantized weights and so is the computation of gradient. \n",
        "- The only parameters are those in PyTorch native layers, so PyTorch will update the floating-point copy of weights.\n",
        "\n",
        "The `ConvBN_Quant` offers the following helper functions:\n",
        "- The `fold` function computes the weights after the `conv` and `bn` are folded and store the folded weights into the parameters of `conv`. Once it is folded, folding won't be performed again when forwarding.\n",
        "- The `foldAndQuantize` function calls `fold` and quantizes the weights of `conv`.\n",
        "\n",
        "The `Linear_Quant` offers the following helper function:\n",
        "- The `quantize` function quantizes the weights of `linear`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnWfIGRnG4z-"
      },
      "source": [
        "class ConvBN_Quant(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, padding=1,nBits=8):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.nBits=nBits\n",
        "        super(ConvBN_Quant, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes,out_planes,kernel_size,stride,padding)\n",
        "        self.bn = nn.BatchNorm2d(out_planes)\n",
        "        self.folded = False\n",
        "        self.quantized = False\n",
        "\n",
        "    def forward(self,x):\n",
        "        r=self.bn.weight.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "        var=self.bn.running_var.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "        \n",
        "        if not self.folded:\n",
        "            w=self.conv.weight*r/(torch.sqrt(var+self.bn.eps))\n",
        "            b=(self.conv.bias-self.bn.running_mean)*self.bn.weight/(torch.sqrt(self.bn.running_var+self.bn.eps))+self.bn.bias\n",
        "        else:\n",
        "            w=self.conv.weight\n",
        "            b=self.conv.bias\n",
        "        if not self.quantized:        \n",
        "            w=FakeQuantOp.apply(w,self.nBits)\n",
        "            b=FakeQuantOp.apply(b,self.nBits)\n",
        "\n",
        "        x=F.conv2d(x,w,b,self.stride,self.padding)\n",
        "        return x\n",
        "\n",
        "    def fold(self):\n",
        "        r=self.bn.weight.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "        var=self.bn.running_var.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
        "        self.conv.weight.data = self.conv.weight.data*r/(torch.sqrt(var+self.bn.eps))\n",
        "        self.conv.bias.data = (self.conv.bias.data-self.bn.running_mean)*self.bn.weight.data/(torch.sqrt(self.bn.running_var+self.bn.eps))+self.bn.bias.data\n",
        "        self.folded = True\n",
        "\n",
        "    def foldAndQuantize(self):\n",
        "        self.fold()\n",
        "        self.quantized = True\n",
        "        self.conv.weight.data = FakeQuantOp.apply(self.conv.weight.data,self.nBits)\n",
        "        self.conv.bias.data = FakeQuantOp.apply(self.conv.bias.data,self.nBits)\n",
        "\n",
        "class Linear_Quant(nn.Module):\n",
        "    def __init__(self,in_features,out_features,nBits=8):\n",
        "        super(Linear_Quant,self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.nBits = nBits\n",
        "        self.fc = nn.Linear(in_features,out_features)\n",
        "        self.quantized = False\n",
        "\n",
        "    def forward(self,x):\n",
        "        w = self.fc.weight\n",
        "        b = self.fc.bias\n",
        "        if not self.quantized:\n",
        "            w = FakeQuantOp.apply(w,self.nBits)\n",
        "            b = FakeQuantOp.apply(b,self.nBits)\n",
        "        x=F.linear(x,w,b)\n",
        "        return x\n",
        "\n",
        "    def quantize(self):\n",
        "        self.quantized = True\n",
        "        self.fc.weight.data = FakeQuantOp.apply(self.fc.weight.data,self.nBits)\n",
        "        self.fc.bias.data = FakeQuantOp.apply(self.fc.bias.data,self.nBits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sc2mC_vLAAe"
      },
      "source": [
        "### Define quantized model\n",
        "With the two layers, we can start to build our quantized model.\n",
        "The model architecture is similar to the floating-point one, the main difference is that we **quantize the weights and activations** in this model.\n",
        "We can specify the bit-width for quantization when instantiating the model by setting `nBits`.\n",
        "- To quantize the activation, we apply `FakeQuantOp` on `x` before it is passed into the next layers. Noting that since the activation function is relu6, we impose the quantization to use 4 bits for the intergeral part.\n",
        "- Also, `'floor'` is used to match the hardware implementation which will be introduced later. If it is changed to `'round'`, the circuit design should be changed accordingly.\n",
        "\n",
        "The helper function `foldAndQuantize` is offered to quantize and/or fold the layers in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz0C8RmbLKoj"
      },
      "source": [
        "class ModelMnist_quant(nn.Module):\n",
        "    def __init__(self,numNeurons=32,nBits=4):\n",
        "        super(ModelMnist_quant, self).__init__()\n",
        "\n",
        "        self.numNeurons = numNeurons\n",
        "        self.nBits = nBits\n",
        "\n",
        "        self.conv1 = ConvBN_Quant(1,3,3,stride=2,padding=0,nBits=nBits)#output 3*13*13\n",
        "        self.fc1 = Linear_Quant(3*13*13, numNeurons,nBits=nBits)\n",
        "        self.fc2 = Linear_Quant(numNeurons, numNeurons,nBits=nBits)\n",
        "        self.fc3 = Linear_Quant(numNeurons,10,nBits=nBits)\n",
        "        self.dropout=nn.Dropout(p=0.2)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x=F.relu6(x)\n",
        "        x=FakeQuantOp.apply(x,self.nBits,self.nBits-4,'floor')\n",
        "\n",
        "        x=x.view(-1,3*13*13)\n",
        "        x=self.dropout(x)\n",
        "        x=self.fc1(x)\n",
        "        x=F.relu6(x)\n",
        "        x=FakeQuantOp.apply(x,self.nBits,self.nBits-4,'floor')\n",
        "        \n",
        "        x=self.fc2(x)\n",
        "        x=F.relu6(x)\n",
        "        x=FakeQuantOp.apply(x,self.nBits,self.nBits-4,'floor')\n",
        "        \n",
        "        x=self.fc3(x)\n",
        "        x=FakeQuantOp.apply(x,self.nBits,self.nBits-4,'floor')\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def foldAndQuantize(self):\n",
        "        self.conv1.foldAndQuantize()\n",
        "        self.fc1.quantize()\n",
        "        self.fc2.quantize()\n",
        "        self.fc3.quantize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-RmSTfACzBD"
      },
      "source": [
        "# Load and Preprocess the Data\n",
        "## Data partition\n",
        "- The original training set of MNIST contains 60000 images.\n",
        "We randomly split them into new training set with 50000 images and validation set with 10000 images.\n",
        "\n",
        "## Data preprocessing\n",
        "- The preprocessing simply normalizes the images with mean = 0.5 and stdandard deviation = 0.5. ⚠ Please **do not** modify them.\n",
        "- Some other common data preprocessing includes:\n",
        "    - [random flip](https://pytorch.org/vision/stable/transforms.html?highlight=flip#torchvision.transforms.RandomHorizontalFlip)\n",
        "    - [random rotation](https://pytorch.org/vision/stable/transforms.html?highlight=rotation#torchvision.transforms.RandomRotation)\n",
        "    - [random crop](https://pytorch.org/vision/stable/transforms.html?highlight=crop#torchvision.transforms.RandomCrop)\n",
        "    - more can be found [here](https://pytorch.org/vision/stable/transforms.html)\n",
        "\n",
        "  Try to add them and see how the accuracy is affected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPWb9Xm4Cibs"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "dataset_all = datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       # add other preprocessing here\n",
        "                       # E.g., transforms.RandomRotation(5),\n",
        "                       transforms.RandomRotation(5),\n",
        "                       transforms.Normalize((0.5,), (0.5,))\n",
        "                   ]))\n",
        "train_set, val_set = torch.utils.data.random_split(dataset_all, [50000, 10000]) # split the dataset\n",
        "# construct the three data loader\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)    \n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.5,), (0.5,))\n",
        "                   ])),\n",
        "    batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai6slZyhDVD9"
      },
      "source": [
        "# Train the Model\n",
        "- Set `device` based on whether the cuda is available.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAO1oo1GbBT8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hypeter parameters for training includes:\n",
        "- `epochs`: number of epochs for training\n",
        "- `lr`: learning rate"
      ],
      "metadata": {
        "id": "MENv1uSfe2-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lr = 0.0005\n",
        "lossFunc=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "slOcdwSBe2Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odh320HcbtTf"
      },
      "source": [
        "\n",
        "\n",
        "The training procedure consists of the following steps:\n",
        "1. Clear the gradient in the previous round.\n",
        "2. Run the forward pass of the model.\n",
        "3. Compute the loss.\n",
        "4. Compute the gradient of the loss.\n",
        "5. Update the weights.\n",
        "\n",
        "The steps are performed iteratively. In each iteration, a batch of data (specified by `batch_size`) is used.\n",
        "\n",
        "The loss function used is cross entropy loss and the optimizer is Adam.\n",
        "\n",
        "We evaluate the model on the validation set for evey 5 epochs.\n",
        "If the validation accuracy is higher than the recorded highest one, we save the model and update the record.\n",
        "\n",
        "It may take about 25 minutes to train for 100 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the quantized model:"
      ],
      "metadata": {
        "id": "AtGnMaH4frIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ModelMnist_quant().to(device)\n",
        "modelSaveName = \"model.pt\"\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
        "bestValidAcc=0\n",
        "for epoch in range(1,epochs+1):\n",
        "    #model.train()\n",
        "    for step,(x,y) in enumerate(train_loader):\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        optimizer.zero_grad() # clear the gradient in the previous round\n",
        "        output=model(x) # run the forward pass of the model\n",
        "        loss=lossFunc(output,y) # compute the loss\n",
        "        loss.backward() # compute the gradient of the loss\n",
        "        optimizer.step() # update the weights\n",
        "    scheduler.step()\n",
        "    if epoch%5==0:\n",
        "        model.eval() # switch the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            correctCount=0\n",
        "            for x,y in train_loader:\n",
        "                x,y=x.to(device),y.to(device)\n",
        "                pred=model(x).max(1)[1] # argmax\n",
        "                correctCount+=torch.sum(pred==y).item()\n",
        "            trainAcc=correctCount/len(train_loader.dataset)\n",
        "            correctCount=0\n",
        "            for x,y in valid_loader:\n",
        "                x,y=x.to(device),y.to(device)\n",
        "                pred=model(x).max(1)[1]# argmax\n",
        "                correctCount+=torch.sum(pred==y).item()\n",
        "            validAcc=correctCount/len(valid_loader.dataset)\n",
        "        model.train()# switch the model to training mode\n",
        "        print(\"epoch:{}, train acc:{}, valid acc:{}\".format(epoch,trainAcc,validAcc))\n",
        "        if validAcc>bestValidAcc:\n",
        "            bestValidAcc=validAcc\n",
        "            torch.save(model,modelSaveName) # save the model to the desired location\n",
        "print(\"best valid acc:\",bestValidAcc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PvEqz60f2wq",
        "outputId": "cd2527b2-82a9-4458-eb7c-8a2c9cdf08e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:5, train acc:0.92936, valid acc:0.9244\n",
            "epoch:10, train acc:0.94998, valid acc:0.9474\n",
            "epoch:15, train acc:0.95374, valid acc:0.9504\n",
            "epoch:20, train acc:0.95714, valid acc:0.9528\n",
            "epoch:25, train acc:0.95736, valid acc:0.9516\n",
            "epoch:30, train acc:0.95796, valid acc:0.953\n",
            "epoch:35, train acc:0.96298, valid acc:0.9576\n",
            "epoch:40, train acc:0.96356, valid acc:0.957\n",
            "epoch:45, train acc:0.96242, valid acc:0.9532\n",
            "epoch:50, train acc:0.9648, valid acc:0.9578\n",
            "epoch:55, train acc:0.96802, valid acc:0.9586\n",
            "epoch:60, train acc:0.9605, valid acc:0.9534\n",
            "epoch:65, train acc:0.96298, valid acc:0.9569\n",
            "epoch:70, train acc:0.96526, valid acc:0.9589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXU02VFwO50O"
      },
      "source": [
        "# Evaluate the Model on the Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq2PW_xbDq_7",
        "outputId": "9645e43c-196a-4812-de7e-8caea33c86f2"
      },
      "source": [
        "model_q = torch.load(\"model.pt\") # load the floating point model with the highest valid accuracy\n",
        "model_q.eval()\n",
        "correctCount=0\n",
        "for x,y in test_loader:\n",
        "    x,y=x.to(device),y.to(device)\n",
        "    pred=model_q(x).max(1)[1]\n",
        "    correctCount+=torch.sum(pred==y).item()\n",
        "testAcc=correctCount/len(test_loader.dataset)\n",
        "print(\"Testing accuracy of the quantized model: \",testAcc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing accuracy of the quantized model:  0.9522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "1. Train `ModelMnist_float` and `ModelMnist_quant` using the settings above.\n",
        "Save the trained models and record the training, validation and testing accuracy in the report.\n",
        "\n",
        "2. Try different learning rates using the quantized model and draw the learning curve. \n",
        "More specifically, set the learning rate to be 0.005, 0.0005, 0.00005 and then record and plot the training and validation accuracy every 5 epochs (train for 150 epochs in total).\n",
        "Plot the three curves in a single plot.\n",
        "\n",
        "3. Try different model architecture built with `ConvBN_Quant` and `Linear_Quant` and try other bit widths. \n",
        "Record your setting and accuracy in the report. Remember to record the bitwidth used for activations as well.\n",
        "\n",
        "    Be careful that the model can’t be to “big”, since the FPGA may be not capable to implement it due to the limited hardware resource. \n",
        "    For your information, for the model in this jupyter-notebook, the `conv1` uses about 29K LUTs, the `fc1` uses about 13K LUTs, the `fc2` uses about 4K LUTs, the `fc3` uses about 3K LUTs and other control circuit takes about 33K LUTs. \n",
        "    The FPGA board we will use has 133.8K LUTs in total. \n",
        "\n",
        "    For training hyper parameters (e.g., learning rate, total epochs, batch size, optimizer, etc.) and data preprocess, you are encouraged to try different settings to achieve higher accuracy. Please record these setting in the report as well. **Note that do not modify the normalization when doing data preprocess. Keep it using mean=0.5 and standard deviation=0.5**."
      ],
      "metadata": {
        "id": "h8kKe4R9i2-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "1. Report\n",
        "    - Record the training, validation and testing accuracy for task 1 and write down your observation briefly.\n",
        "    - Put the figure of task 2 in the report and write down your observation briefly.\n",
        "    - Record the settings used for task 3 and the resulting training, validation and testing accuracy. Also, write down your observation briefly.\n",
        "    - You can write down your observation, thoughts, suggestions or something else you want to say in the report.\n",
        "\n",
        "\n",
        "2. Saved models\n",
        "    - The floating-point model in task 1. Please name it “mnistModel_float.pt”.\n",
        "    - The quantized model in task 1. Please name it “mnistModel_quant.pt”.\n",
        "    - The model trained in task 3. Please name it “mnistModel_quant_modified.pt”\n",
        "\n",
        "## Hierarchy\n",
        "Please put all the files in a folder named `lab1_studentID` with the following hierarchy:\n",
        "\n",
        "- lab1_studentID/\n",
        "    - report.pdf\n",
        "    - models/\n",
        "        - mnistModel_float.pt\n",
        "        - mnistModel_quant.pt\n",
        "        - mnistModel_quant_modified.pt\n",
        "\n",
        "Please compress the top folder into a single zip file named `lab1_studentID.zip`.\n"
      ],
      "metadata": {
        "id": "HTPHX_pmlvLB"
      }
    }
  ]
}